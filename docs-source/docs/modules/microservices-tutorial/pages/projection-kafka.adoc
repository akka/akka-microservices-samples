= VII: Projection publishing to Kafka
:page-supergroup-java-scala: Language

include::ROOT:partial$include.adoc[]

In this part, we will add another Projection from the events of the `ShoppingCart` entity. This Projection publishes the events to a Kafka message broker. We will also add another service, the `ShoppingAnalyticsService` that consumes the events from the Kafka topic.

.This part of the xref:overview.adoc[full example] will focus on the Kafka producer in the `PublishEventsProjection` and Kafka consumer in `ShoppingAnalyticsService`.
[caption=""]
image::example-projection-kafka.png[Example Kafka]

As described in xref:concepts:internal-and-external-communication.adoc[Internal and External Communication concepts] we can publish messages to a broker, such as Apache Kafka, to decouple communication between different Microservices.

On this page you will learn how to:

* send messages to a Kafka topic from a Projection
* consume messages from a Kafka topic

== Source downloads

If you prefer to simply view and run the example, download a zip file containing the completed code:

[.tabset]
Java::
+
****
* link:_attachments/4-shopping-cart-projection-java.zip[Source] that includes all previous tutorial steps and allows you to start with the steps on this page.
* link:_attachments/5-shopping-cart-projection-kafka-java.zip[Source] with the steps on this page completed.
****

Scala::
+
****
* link:_attachments/4-shopping-cart-projection-scala.zip[Source] that includes all previous tutorial steps and allows you to start with the steps on this page.
* link:_attachments/5-shopping-cart-projection-kafka-scala.zip[Source] with the steps on this page completed.
****

== External representation of the events

For external APIs of a service, such as a Kafka topic that is consumed by other services, it is good to have a well defined format of the data. Therefore we define that in Protobuf, rather than using the internal representation of the events. This also makes it easier to evolve representation over time without breaking downstream consumers.

Add a new `ShoppingCartEvents.proto` with the specification of the events:

[source,protobuf]
----
include::example$05-shopping-cart-service-scala/src/main/protobuf/ShoppingCartEvents.proto[]
----

Generate code by compiling the project:

[.group-scala]
[source,shell script]
----
sbt compile
----

[.group-java]
[source,shell script]
----
mvn compile
----

== Sending to Kafka from a Projection

The new Projection will be similar to what we developed in the xref:projection-query.adoc[previous step], but it will send the events to a Kafka topic instead of updating a database.

Add a `PublishEventsProjectionHandler` class that is the Projection `Handler` for processing the events:

[.tabset]
Java::
+
.src/main/java/shopping/cart/PublishEventsProjectionHandler.java:
[source,java,indent=0]
----
include::example$05-shopping-cart-service-java/src/main/java/shopping/cart/PublishEventsProjectionHandler.java[tag=handler]
----

Scala::
+
.src/main/scala/shopping/cart/PublishEventsProjectionHandler.scala:
[source,scala,indent=0]
----
include::example$05-shopping-cart-service-scala/src/main/scala/shopping/cart/PublishEventsProjectionHandler.scala[tag=handler]
----

<1> `SendProducer` comes from the Kafka connector in Alpakka.
<2> The events are serialized to Protobuf and sent to the given topic.
<3> Wrap in Protobuf `Any` to include type information.

The serialization converts the `ShoppingCart.Event` classes to the Protobuf representation. Since several types of messages are sent to the same topic we must include some type information that the consumers of the topic can use when deserializing the messages. Protobuf provides a built-in type called `Any` for this purpose. [.group-scala]#That is why it is wrapped with `ScalaPBAny.pack`.#

== Initialize the Projection

The tagging of the events is already in place from the xref:projection-query.adoc#tagging[previous step].

Place the initialization code of the Projection in an `PublishEventsProjection` [.group-scala]#object# [.group-java]#class#:

[.tabset]
Java::
+
.src/main/java/shopping/cart/PublishEventsProjection.java:
[source,java,indent=0]
----
include::example$05-shopping-cart-service-java/src/main/java/shopping/cart/PublishEventsProjection.java[]
----

Scala::
+
.src/main/scala/shopping/cart/PublishEventsProjection.scala:
[source,scala,indent=0]
----
include::example$05-shopping-cart-service-scala/src/main/scala/shopping/cart/PublishEventsProjection.scala[]
----

The `SendProducer` is initialized using some configuration that we need to add. It defines how to connect to the Kafka broker.

Add the following to a new `src/main/resources/kafka.conf` file:

[source,hocon]
----
include::example$05-shopping-cart-service-scala/src/main/resources/kafka.conf[]
----

Include `kafka.conf` from `application.conf`.

And for local development add the following to `src/main/resources/local-shared.conf`, which is loaded when running locally:

[source,hocon]
----
include::example$05-shopping-cart-service-scala/src/main/resources/local-shared.conf[tag=kafka]
----

Then we need to call the `PublishEventsProjection.init` from `Main`:

[.tabset]
Java::
+
[source,java,indent=0]
----
include::example$05-shopping-cart-service-java/src/main/java/shopping/cart/Main.java[tag=PublishEventsProjection]
----

Scala::
+
[source,scala,indent=0]
----
include::example$05-shopping-cart-service-scala/src/main/scala/shopping/cart/Main.scala[tag=PublishEventsProjection]
----

== Consuming the events

=== New project

Let's add another service that consumes the events from the Kafka topic.

The xref:template.adoc#seed-template[template download] (or other source downloads) includes a directory named `shopping-analytics-service`. Open the `shopping-analytics-service` xref:template.adoc#intellij[in IntelliJ in the same way as you did with the shopping-cart-service].

=== Protobuf

This service will receive the events in the Protobuf format defined in the `ShoppingCartEvents.proto` from the `shopping-cart-service`. Copy that file to the `shopping-analytics-service/src/main/protobuf` and generate code by compiling the project:

[.group-scala]
[source,shell script]
----
sbt compile
----

[.group-java]
[source,shell script]
----
mvn compile
----

Note that different services should not share code, but the Protobuf specification can be copied since that is the published interface of the service.

=== Consumer

Create a `ShoppingCartEventConsumer` [.group-scala]#object# [.group-java]#class# in `shopping-analytics-service`. It runs an Akka Stream with a Kafka `Consumer.committableSource` from Alpakka Kafka.

[.tabset]
Java::
+
.src/main/java/shopping/analytics/ShoppingCartEventConsumer.java:
[source,java,indent=0]
----
include::example$shopping-analytics-service-java/src/main/java/shopping/analytics/ShoppingCartEventConsumer.java[tag=consumer]
----

Scala::
+
.src/main/scala/shopping/analytics/ShoppingCartEventConsumer.scala:
[source,scala,indent=0]
----
include::example$shopping-analytics-service-scala/src/main/scala/shopping/analytics/ShoppingCartEventConsumer.scala[tag=consumer]
----

<1> `RestartSource` will restart the stream in case of failures.

<2> Kafka Consumer stream.

<3> Offset is committed to Kafka when records have been processed.

<4> Protobuf `Any` for type information.

Note how the deserialization is using the type information from the Protobuf `Any` to decide which type of event to deserialize.

=== Configuration

The `Consumer` is initialized using some configuration that we need to add. It defines how to connect to the Kafka broker.

Add the following to a new `src/main/resources/kafka.conf` file in `shopping-analytics-service`:

[source,hocon]
----
include::example$shopping-analytics-service-scala/src/main/resources/kafka.conf[]
----

Include `kafka.conf` from `application.conf`.

And for local development add the following to `src/main/resources/local-shared.conf`, which is loaded when running locally:

[source,hocon]
----
include::example$shopping-analytics-service-scala/src/main/resources/local-shared.conf[tag=kafka]
----

=== Main

Edit the `Main` class that is included from the template project. It should initialize the `ActorSystem` and the `ShoppingCartEventConsumer` like this:

[.tabset]
Java::
+
[source,java,indent=0]
----
include::example$shopping-analytics-service-java/src/main/java/shopping/analytics/Main.java[]
----

Scala::
+
[source,scala,indent=0]
----
include::example$shopping-analytics-service-scala/src/main/scala/shopping/analytics/Main.scala[]
----

== Run locally

In addition to Cassandra we now also need Kafka. Kafka is also started by the `docker-compose` script.

Start Cassandra and Kafka, unless it's already running, from `shopping-cart-service`:

[source,shell script]
----
docker-compose up -d
----

Run the `shopping-cart-service` with:

[.group-java]
[source,shell script]
----
# make sure to compile before running exec:exec
mvn compile exec:exec -DAPP_CONFIG=local1.conf
----

[.group-scala]
[source,shell script]
----
sbt -Dconfig.resource=local1.conf run
----

Run the new `shopping-analytics-service` with:

[.group-java]
[source,shell script]
----
# make sure to compile before running exec:exec
mvn compile exec:exec -DAPP_CONFIG=local1.conf
----

[.group-scala]
[source,shell script]
----
sbt -Dconfig.resource=local1.conf run
----

=== Exercise the service

Try it with `grpcurl`. Add 1 pencil to a cart:

[source,shell script]
----
grpcurl -d '{"cartId":"cart1", "itemId":"pencil", "quantity":1}' -plaintext 127.0.0.1:8101 shoppingcart.ShoppingCartService.AddItem
----

Look at the log output in the terminal of the `shopping-analytics-service`. There you should see the logging from the ``:

----
ItemAdded: 1 pencil to cart cart1
----

=== Stop the service

When finished, stop the `shopping-cart-service` and `shopping-analytics-service` with `ctrl-c`. Leave Cassandra and Kafka running for the next set of steps, or stop them with:

[source,shell script]
----
docker-compose down
----

== Learn more

* xref:concepts:internal-and-external-communication.adoc[Internal and External Communication concepts].
* {akka-projection}/[Akka Projection reference documentation {tab-icon}, window="tab"].
